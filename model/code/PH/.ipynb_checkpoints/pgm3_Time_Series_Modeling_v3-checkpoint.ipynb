{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#--- Import Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#--- Import Sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#--- Importing tqdm for the progress bar\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ PICKLE OF THE ORIGINAL AND DIFFERENCED SERIES\n",
    "Dengue_PH = pd.read_pickle('C:/Users/Claire/Documents/GitHub/nasa_hack/model/datasets/Dengue_PH_Clean.pickle')\n",
    "Dengue_PH_diff = pd.read_pickle('C:/Users/Claire/Documents/GitHub/nasa_hack/model/datasets/Dengue_PH_Diff.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GTrend_Dengue', 'GTrend_Dengue_Fever', 'GTrend_Dengue_Cure',\n",
       "       'GTrend_Dengue_Med', 'GTrend_Dengue_Sym', 'Mort_Rate', 'MTD_Cases',\n",
       "       'Reg_Ave_Temp_NCR', 'Reg_Ave_Rainfall_NCR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dengue_PH_diff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE LAGGED VERSIONS OF THE PREDICTORS (MAX OF A QUARTER)\n",
    "Dengue_PH_diff['Temp_L1'] = Dengue_PH_diff['Reg_Ave_Temp_NCR'].shift(1)\n",
    "Dengue_PH_diff['Temp_L2'] = Dengue_PH_diff['Reg_Ave_Temp_NCR'].shift(2)\n",
    "Dengue_PH_diff['Temp_L3'] = Dengue_PH_diff['Reg_Ave_Temp_NCR'].shift(3)\n",
    "\n",
    "Dengue_PH_diff['Rain_L1'] = Dengue_PH_diff['Reg_Ave_Rainfall_NCR'].shift(1)\n",
    "Dengue_PH_diff['Rain_L2'] = Dengue_PH_diff['Reg_Ave_Rainfall_NCR'].shift(2)\n",
    "Dengue_PH_diff['Rain_L3'] = Dengue_PH_diff['Reg_Ave_Rainfall_NCR'].shift(3)\n",
    "\n",
    "Dengue_PH_diff['GT_Dengue_L1'] = Dengue_PH_diff['GTrend_Dengue'].shift(1)\n",
    "Dengue_PH_diff['GT_Dengue_L2'] = Dengue_PH_diff['GTrend_Dengue'].shift(2)\n",
    "Dengue_PH_diff['GT_Dengue_L3'] = Dengue_PH_diff['GTrend_Dengue'].shift(3)\n",
    "\n",
    "Dengue_PH_diff['GT_DengueFvr_L1'] = Dengue_PH_diff['GTrend_Dengue_Fever'].shift(1)\n",
    "Dengue_PH_diff['GT_DengueFvr_L2'] = Dengue_PH_diff['GTrend_Dengue_Fever'].shift(2)\n",
    "Dengue_PH_diff['GT_DengueFvr_L3'] = Dengue_PH_diff['GTrend_Dengue_Fever'].shift(3)\n",
    "\n",
    "Dengue_PH_diff['GT_DengueCure_L1'] = Dengue_PH_diff['GTrend_Dengue_Cure'].shift(1)\n",
    "Dengue_PH_diff['GT_DengueCure_L2'] = Dengue_PH_diff['GTrend_Dengue_Cure'].shift(2)\n",
    "Dengue_PH_diff['GT_DengueCure_L3'] = Dengue_PH_diff['GTrend_Dengue_Cure'].shift(3)\n",
    "\n",
    "Dengue_PH_diff['GT_DengueMed_L1'] = Dengue_PH_diff['GTrend_Dengue_Med'].shift(1)\n",
    "Dengue_PH_diff['GT_DengueMed_L2'] = Dengue_PH_diff['GTrend_Dengue_Med'].shift(2)\n",
    "Dengue_PH_diff['GT_DengueMed_L3'] = Dengue_PH_diff['GTrend_Dengue_Med'].shift(3)\n",
    "\n",
    "Dengue_PH_diff['GT_DengueSym_L1'] = Dengue_PH_diff['GTrend_Dengue_Sym'].shift(1)\n",
    "Dengue_PH_diff['GT_DengueSym_L2'] = Dengue_PH_diff['GTrend_Dengue_Sym'].shift(2)\n",
    "Dengue_PH_diff['GT_DengueSym_L3'] = Dengue_PH_diff['GTrend_Dengue_Sym'].shift(3)\n",
    "\n",
    "#Dengue_PH_diff['Cases_L1'] = Dengue_PH_diff['MTD_Cases'].shift(1)\n",
    "#Dengue_PH_diff['Cases_L2'] = Dengue_PH_diff['MTD_Cases'].shift(2)\n",
    "#Dengue_PH_diff['Cases_L3'] = Dengue_PH_diff['MTD_Cases'].shift(3)\n",
    "\n",
    "Dummies = pd.get_dummies(Dengue_PH_diff.index.month, prefix='m')\n",
    "Dengue_PH_diff = Dengue_PH_diff.reset_index()\n",
    "Dengue_PH_diff = Dengue_PH_diff.merge(Dummies, left_index=True, right_index=True)\n",
    "Dengue_PH_diff.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 42)\n",
      "(12, 42)\n"
     ]
    }
   ],
   "source": [
    "# SPLIT SERIES TO TRAINING AND TEST SETS\n",
    "#--- Set 2018 as the test dataframe\n",
    "nobs = 12\n",
    "df_train, df_test = Dengue_PH_diff[0:-nobs], Dengue_PH_diff[-nobs:]\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Check size\n",
    "print(df_train.shape)  \n",
    "print(df_test.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Temp_L3',\n",
       " 'Rain_L3',\n",
       " 'GT_Dengue_L1',\n",
       " 'GT_DengueFvr_L1',\n",
       " 'GT_DengueMed_L1',\n",
       " 'GT_DengueSym_L1',\n",
       " 'm_2',\n",
       " 'm_3',\n",
       " 'm_4',\n",
       " 'm_5',\n",
       " 'm_6',\n",
       " 'm_7',\n",
       " 'm_8',\n",
       " 'm_9',\n",
       " 'm_10',\n",
       " 'm_11',\n",
       " 'm_12']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERFORM UNIVARIATE REGRESSION TO TRIM DOWN THE PREDICTORS\n",
    "predictor_col = df_train.columns[df_train.columns.str.contains(pat = '_L')]\n",
    "pvals = pd.DataFrame()\n",
    "for col in predictor_col:\n",
    "    Y = df_train.MTD_Cases\n",
    "    X = df_train[col]\n",
    "    X2 = sm.add_constant(X)\n",
    "    mod = sm.OLS(Y,X)\n",
    "    fit = mod.fit()\n",
    "    pval = fit.summary2().tables[1]['P>|t|']\n",
    "    pval = pval.to_frame()\n",
    "    pvals = pvals.append(pval)\n",
    "\n",
    "# RETAIN ONLY THE LAGGED PREDICTORS WITH SIGNIFICANT P-VALUES\n",
    "pvals = pvals[pvals['P>|t|'] <= 0.05].reset_index()\n",
    "pvals = pvals.rename(columns={'index':'Variable'})\n",
    "shortlist_predictor_col = pvals['Variable']\n",
    "dummy = pd.Series(df_train.columns[df_train.columns.str.startswith('m_')])\n",
    "shortlist_predictor_col = shortlist_predictor_col.append(dummy)\n",
    "X = df_train[shortlist_predictor_col].drop(columns=['m_1'],axis=1)\n",
    "#X = df_train[shortlist_predictor_col]\n",
    "list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model \n",
    "    X2 = sm.add_constant(X)\n",
    "    model_k = sm.OLS(Y,X2)\n",
    "    fit = model_k.fit()\n",
    "    pval = fit.pvalues.to_frame()\n",
    "    features = list(pval.index)\n",
    "    pvals = list(pval[0])\n",
    "    sig = pval[pval[0]<=0.05]\n",
    "    pct_sig = len(list(sig[0])) / len(list(pval[0]))\n",
    "    rsq = fit.rsquared\n",
    "    adjr = fit.rsquared_adj\n",
    "    serial_corr = list(sm.stats.diagnostic.acorr_breusch_godfrey(fit, nlags=3))[3]\n",
    "    het_arch = list(sm.stats.diagnostic.het_arch(fit.resid, maxlag=1))[3]\n",
    "    normality = list(sm.stats.stattools.jarque_bera(fit.resid))[1]\n",
    "    Y_Pred_Test = fit.predict(sm.add_constant(df_test[list(X)]))\n",
    "    mae = mean_absolute_error(Y_True_Test,Y_Pred_Test)\n",
    "    mse = mean_squared_error(Y_True_Test,Y_Pred_Test)\n",
    "    rmse = sqrt(mean_squared_error(Y_True_Test,Y_Pred_Test))\n",
    "    return features, pvals, pct_sig, rsq, adjr, serial_corr, het_arch, normality, mae, mse, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE VARIABLES\n",
    "Y = df_train.MTD_Cases\n",
    "Y_True_Test = df_test.MTD_Cases\n",
    "#X = df_train[df_train.columns[df_train.columns.str.contains(r'_L|m_')]].drop(columns='m_1')\n",
    "#X = df_train[df_train.columns[df_train.columns.str.contains(r'_L')]]\n",
    "k = 8\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "R_squared_list, AdjR2_list, feature_list, pval_list = [],[],[],[]\n",
    "pct_sig_list = []\n",
    "num_features = []\n",
    "serial_corr_list = []\n",
    "het_arch_list = []\n",
    "norm_list = []\n",
    "mae_list, mse_list, rmse_list = [],[],[]\n",
    "\n",
    "# RUN FORWARD STEPWISE REGRESSION TO GET THE BEST SUBSET MODEL PER NUMBER OF PREDICTORS\n",
    "#--- Loop over k = 1 to k = 10 features in X\n",
    "#for k in tnrange(1, k+1, desc = 'Loop...'):\n",
    "while (len(remaining_features) > 1):\n",
    "    #--- Initialize adjusted R2\n",
    "    best_adjr = 0\n",
    "\n",
    "    #--- Add predictors one at a time.  At each step, the predictor that gives the greatest improvement to adjusted R2 is added\n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "        #--- Store temp result \n",
    "        tmp_result = fit_linear_reg(X[list(set(remaining_features) - set(combo))],Y)  \n",
    "        \n",
    "        #if tmp_result[5] > 0.05:\n",
    "            #if tmp_result[6] > 0.05:\n",
    "                #if tmp_result[7] > 0.05:\n",
    "        if tmp_result[4] > best_adjr:\n",
    "                best_combo = combo[0]\n",
    "                best_features = tmp_result[0]\n",
    "                best_pvals = tmp_result[1]\n",
    "                best_pct = tmp_result[2]\n",
    "                best_r2 = tmp_result[3]\n",
    "                best_adjr2 = tmp_result[4]\n",
    "                best_scorr = tmp_result[5]\n",
    "                best_het = tmp_result[6]\n",
    "                best_norm = tmp_result[7]\n",
    "                best_mae = tmp_result[8]\n",
    "                best_mse = tmp_result[9]\n",
    "                best_rmse = tmp_result[10]\n",
    "    \n",
    "    #--- Update predictors for next loop\n",
    "    features.append(best_combo)\n",
    "    remaining_features.remove(best_combo)\n",
    "    \n",
    "    #--- Append List\n",
    "    num_features.append(len(best_features)-1) \n",
    "    feature_list.append(best_features)\n",
    "    pval_list.append(best_pvals)\n",
    "    pct_sig_list.append(best_pct)\n",
    "    R_squared_list.append(best_r2)\n",
    "    AdjR2_list.append(best_adjr2)\n",
    "    serial_corr_list.append(best_scorr)\n",
    "    het_arch_list.append(best_het)\n",
    "    norm_list.append(best_norm)\n",
    "    mae_list.append(best_mae)\n",
    "    mse_list.append(best_mse)\n",
    "    rmse_list.append(best_rmse)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 0.21766481762962941),\n",
       " (15, 0.07945995716887566),\n",
       " (14, 0.08108162675100639),\n",
       " (13, 0.11849984218278975),\n",
       " (12, 0.15027044319119798),\n",
       " (11, 0.19275006666722483),\n",
       " (10, 0.22907923497918925),\n",
       " (9, 0.18845749724691196),\n",
       " (8, 0.22217167859794262),\n",
       " (7, 0.2514848635170749),\n",
       " (6, 0.2789313804553658),\n",
       " (5, 0.2786718747802919),\n",
       " (4, 0.23852868982540487),\n",
       " (3, 0.2233702814969467),\n",
       " (2, 0.17549815930956414),\n",
       " (1, 0.15931595287520872)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(num_features,AdjR2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORE IN DATAFRAME\n",
    "subsets_df = pd.DataFrame({'num_features': num_features, 'features': feature_list, 'P>|t|': pval_list, 'pct_sig': pct_sig_list, \\\n",
    "                          'rsq': R_squared_list, 'adj_rsq': AdjR2_list, 'serial_corr': serial_corr_list, 'het': het_arch_list, \\\n",
    "                          'normality': norm_list, 'mae': mae_list, 'mse': mse_list, 'rmse': rmse_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjR2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETAIN ONLY THE SUBSET MODELS WHICH PASSED THE DIAGNOSTIC TESTS\n",
    "subsets_df2 = subsets_df[(subsets_df['serial_corr'] <= 0.05) & (subsets_df['het'] <= 0.05) & (subsets_df['normality'] <= 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.drop(columns='Temp_L3')\n",
    "#X2 = sm.add_constant(X)\n",
    "est = sm.OLS(Y, X)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X,Y)\n",
    "\n",
    "#For retrieving the slope:\n",
    "print(reg.coef_)\n",
    "\n",
    "prediction = pd.DataFrame(reg.predict(X))\n",
    "prediction.columns = ['Pred_MTD_Cases']\n",
    "df_train2 = df_train.reset_index().join(prediction, how='inner')\n",
    "\n",
    "prediction = pd.DataFrame(reg.predict(df_test[X.columns]))\n",
    "prediction.columns = ['Pred_MTD_Cases']\n",
    "df_test2 = df_test.reset_index().join(prediction, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = df_train2['MTD_Cases']\n",
    "y_pred = df_train2['Pred_MTD_Cases']\n",
    "print(mean_squared_error(y_true, y_pred))\n",
    "print(mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_test2['MTD_Cases']\n",
    "y_pred = df_test2['Pred_MTD_Cases']\n",
    "print(mean_squared_error(y_true, y_pred))\n",
    "print(mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dengue_PH_Fct = df_train2.append(df_test2, ignore_index=True)\n",
    "Dengue_PH_Fct = Dengue_PH_Fct[['Date','Pred_MTD_Cases']]\n",
    "Dengue_PH_Fct['Pred_MTD_Cases'] += 1\n",
    "Dengue_PH_Fct = Dengue_PH_Fct.rename(columns={'Pred_MTD_Cases': 'Pred_Cases_PctChg'})\n",
    "Dengue_PH_Fct.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dengue_PH2 = Dengue_PH.merge(Dengue_PH_Fct,how='left',on='Date')\n",
    "Dengue_PH2['MTD_Cases_Fct'] = Dengue_PH2.MTD_Cases.shift(1) * Dengue_PH2['Pred_Cases_PctChg']\n",
    "Dengue_PH2[['MTD_Cases','Pred_Cases_PctChg','MTD_Cases_Fct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dengue_PH2.to_excel('C:/Users/Claire/Documents/GitHub/nasa_hack/model/datasets/Dengue_PH_Fct.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est2.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
